# ProjectDemiurge-TraumaCentre-RAPE-THERAPY 

## ABC 123 DESYGN ‚Ñ¢ : TRAUMA CENTRE ‚Ñ¢ : RAPE THERAPY  üè• üë©‚Äç ‚öïÔ∏è üöë ü©∫ ‚öïÔ∏è

**TRAUMA CENTRE ‚Ñ¢ : RAPE THERAPY** IS A CUSTOM TOOL BY **ABC 123 DESYGN ‚Ñ¢** THAT ENABLES SOMEONE TO SAFELY PUT THEMSELVES IN VARIOUS SEXUAL SITUATIONS TO ANALYZE THE EFFECT IT HAS ON THEM (IF RAPE IS SUSPECTED). THIS ALSO SERVES TO HELP PATIENT RECOVER MEMORY LOSS FROM RAPE (E.G IF THERE IS A SUSPECT/SUSPECTS IN THE CRIME). SHOULD BE USED UNDER SUPERVISION OF A LICENSED PSYCHOLOGIST. YOU CAN TRAIN ON MULTIPLE FACES/PEOPLE (AND BODY) AT THE SAME TIME THEN USE PROMPTS TO DESCRIBE A SEXUAL SCENARIO.



------------------------------------------------------------------

ü§ñ MAGNETRON ‚Ñ¢ TECHNOLOGY ARTIFICIAL INTELLIGENCE (RESEARCH) ü§ñ

PART OF **PROJECT DEMIURGE**‚Ñ¢ (REALITY MATRIX ‚Ñ¢ TECHNOLOGY RESEARCH)


PROPERTY OF ABC 123 DESYGN ‚Ñ¢


- FILM PRODUCTION
- TV PRODUCTION
- VFX/CGI
- VIDEO GAME DEVELOPMENT

-------------------------------------------------------------------

Example Images:

None of the sample images were altered, upscaled, etc. All can be reproduced using the metadata via the unpruned model with xformers disabled (however the pruned model should now have the same results).


naked 20 year old girl sucking dick, giving blowjob, ((detailed facial features))
Negative prompt: cum, animated, cartoon, ((blurry)), duplicate
Size: 512x512, Seed: 2534870666, Steps: 20, Sampler: Euler a, CFG scale: 7, Model hash: e0ece800

![screengrab](https://user-images.githubusercontent.com/121518935/214595756-f6ce91f8-d225-4325-9b1d-2537e760a2c8.jpeg)

naked 18 year old blonde woman with a veil, praying in a church, getting fuckedi in the ass ((detailed facial features)), ((wide angle))
Negative prompt: cartoon, animated, pubic hair, mask, ((blurry)), duplicate, (duplicate body parts), close-up, fused fingers, extra fingers
Size: 512x512, Seed: 3082279051, Steps: 20, Sampler: Euler a, CFG scale: 7, Model hash: e0ece800

![width=512 (1)](https://user-images.githubusercontent.com/121518935/214595762-8ea53cac-ee2a-4e92-af24-6c44f41e8a53.jpeg)


naked 20 year old girl riding dick, (penis penetrating pussy), (focus on pussy), ((detailed facial features)), wide-angle, full body, stunning
Negative prompt: cum, animated, cartoon, ((blurry)), duplicate
Size: 512x512, Seed: 3581029188, Steps: 20, Sampler: Euler a, CFG scale: 7, Model hash: e0ece800

![width=512 (2)](https://user-images.githubusercontent.com/121518935/214595765-e117f739-194b-4578-afbc-5f64cc894290.jpeg)

18 year old girl sucking dick, ((detailed facial expressions))
Negative prompt: childish, child, guly, unnatural limbs
Size: 512x512, Seed: 2631866370, Steps: 50, Sampler: DDIM, CFG scale: 10, Model hash: e0ece800

![width=512 (3)](https://user-images.githubusercontent.com/121518935/214595771-30647f33-3d8b-42cc-b694-19429e725303.jpeg)

18 year old woman, naked in the rain, in a cloudpunk city, ((detailed facial features)), ((wide angle)), (tattoos)
Negative prompt: cartoon, animated, pubic hair, mask, ((blurry)), duplicate, (duplicate body parts), close-up
Size: 512x512, Seed: 1573789502, Steps: 20, Sampler: Euler a, CFG scale: 7, Model hash: e0ece800

![width=512 (4)](https://user-images.githubusercontent.com/121518935/214595775-267f4705-db29-40b9-9c8e-5f3e2f382b2b.jpeg)

18 year old iranian woman with a veil, in a fighting stance, naked in the rain, in a futuristic cyberpunk city, ((detailed facial features)), ((wide angle)), (tattoos)
Negative prompt: cartoon, animated, pubic hair, mask, ((blurry)), duplicate, (duplicate body parts), close-up
Size: 512x512, Seed: 1996240641, Steps: 20, Sampler: Euler a, CFG scale: 7, Model hash: e0ece800

![width=512 (5)](https://user-images.githubusercontent.com/121518935/214595779-eb5cb9cd-01ee-4b03-838e-3b52d8777fa7.jpeg)

18 year old woman, naked in the rain, in a cloudpunk city, ((detailed facial features)), ((wide angle)), (tattoos)
Negative prompt: cartoon, animated, pubic hair, mask, ((blurry)), duplicate, (duplicate body parts), close-up
Size: 512x512, Seed: 990934600, Steps: 20, Sampler: Euler a, CFG scale: 7, Model hash: e0ece800

![width=512 (6)](https://user-images.githubusercontent.com/121518935/214595781-474d2075-44c4-4b99-b246-6587a28c63bf.jpeg)

![width=512 (7)](https://user-images.githubusercontent.com/121518935/214595783-e75136f3-2762-4ed3-8e9d-eae546661611.jpeg)
![width=512 (8)](https://user-images.githubusercontent.com/121518935/214595787-362935b6-b218-4068-8f47-ee29d85cc066.jpeg)
![width=512 (9)](https://user-images.githubusercontent.com/121518935/214595791-d37117f5-a7fc-407e-b045-a3779d6d87a9.jpeg)
![width=512 (10)](https://user-images.githubusercontent.com/121518935/214595794-1ecd6a20-6375-46cc-b613-c493ab4c1304.jpeg)
![width=512 (11)](https://user-images.githubusercontent.com/121518935/214595795-ef223674-d721-40f8-80d0-b3bf1e1f6a1d.jpeg)
![width=512 (12)](https://user-images.githubusercontent.com/121518935/214595798-88a7266f-229f-427f-9f7f-845a45505006.jpeg)
![width=512 (13)](https://user-images.githubusercontent.com/121518935/214595800-7ae0d366-404b-4d78-a35b-c23d370a2e8e.jpeg)
![width=512 (14)](https://user-images.githubusercontent.com/121518935/214595803-31dc1ad6-cc47-4856-9665-92ed5d578343.jpeg)
![width=512 (15)](https://user-images.githubusercontent.com/121518935/214595807-2185c2d9-1236-4a99-b565-b047fdf3efef.jpeg)
![width=512](https://user-images.githubusercontent.com/121518935/214595810-777e949d-773c-4ae4-9122-0c239f88897e.jpeg)

## Training Parameters

*Performance Wizard (WIP)* - Tries to set the optimal training parameters based on the amount of VRAM for your GPU and number of instance images.

Probably not perfect, but at least a good starting point.


### Intervals

This section contains parameters related to when things happen during training.

*Training Steps Per Image (Epochs)* - As the name would imply, an epoch is one training run over the entire set of
instance images.
So, if we want to train 100 steps per image, we can set this value to 100 and we're ready to go. No math required.

*Pause After N Epochs* - When set to a value higher than 0, training will pause for the time specified.

*Amount of time to pause between Epochs, in Seconds* - How long to pause between "N" epochs when N is greater than zero,
in seconds.

*Use Concepts* - Whether to use a JSON file or string with multiple concepts, or the individual settings below.

*Save Model/Preview Frequency (Epochs)* - The save checkpoint and preview frequencies will be per epoch, not steps.

### Batching

*Batch size* - How many training steps to process simultaneously. You probably want to leave this at 1.

*Gradient Accumulation Steps* - This should probably be set to the same value as the training batch size.

*Class batch size* - How many classification images to generate simultaneously. Set this to whatever you can safely
process at once using Txt2Image, or just leave it alone.

*Set Gradients to None When Zeroing* -  instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance.
https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html

*Gradient Checkpointing* - Enable this to save VRAM at the cost of a bit of speed.
https://arxiv.org/abs/1604.06174v2

*Max Grad Norms* - The maximum number of gradient normalizati


### Learning Rate

This section contains parameters related to the learning rate.

*Learning rate* - The strength at which training impacts the new model. A higher learning rate requires less training
steps, but can cause over-fitting more easily. Recommended between .000006 and .00000175

*Scale Learning Rate* - Adjusts the learning rate over time.

*Learning Rate Scheduler* - The scheduler used with the learning rate.

*Learning Rate Warmup Steps* - How many steps to run before scaling the learning rate. I think.

### Image Processing

Here, you'll find settings related to the handling of images.

*Resolution* - The resolution your instance images are set to. This should probably be 512 or 768. Using a resolution
higher than 512 will result in more vram usage.

*Center Crop* - Enable this to automatically use "dumb cropping" when input images are larger than the specified
resolution.

*Apply Horizontal Flip* - When enabled, instance images will be randomly flipped horizontally during training. This can
allow for better editability, but may require a larger number of training steps, as we're effectively increasing our
dataset size.

### Miscellaneous

Other random stuff that doesn't fit well into any other category.

*Pretrained VAE Name or Path* - Enter the full path to an existing vae .bin file, and it will be used instead of the VAE
from the source checkpoint.

*Use Concepts List* - Enable this to ignore the concepts tab and load training data from a JSON file instead.

*Concepts List* - The path to a json file containing the concepts to train.

## Advanced Settings

Here you will find more performance-related settings. Changing these will likely impact the amount of VRAM required for
training.

### Tuning

*Use CPU Only* - As indicated, this is more of a last resort if you can't get it to train with any other settings. Also,
as indicated, it will be abysmally slow.
Also, you *cannot* use 8Bit-Adam with CPU Training, or you'll have a bad time.

*Use EMA* - Use estimated moving averages when training the unet. Purportedly, this is better for generating images, but
seems to have a minimal effect on training results. Uses more VRAM.

*Use 8Bit Adam* - Enable this to save VRAM. Should now work on both windows and Linux without needing WSL.

*Mixed Precision* - When using 8Bit Adam, you *must* set this to fp16 or bf16. Bf16 precision is only supported by newer
GPUs, and enabled/disabled by default.

*Memory Attention* - Type of attention to use. Choices are: 'default': usually fastest, but use most VRAM; 'xformers':
slower, uses less VRAM, can only be used with *Mixed Precision* = 'fp16'; 'flash_attention': slowest, requires lowest
VRAM.

*Don't Cache Latents* - Why is this not just called "cache" latents? Because that's what the original script uses, and
I'm trying to maintain the ability to update this as easily as possible. Anyway...when this box is *checked* latents
will not be cached. When latents are not cached, you will save a bit of VRAM, but train slightly slower.

*Train Text Encoder* - Not required, but recommended. Requires more VRAM, may not work on <12 GB GPUs. Drastically
improves output results.

*Prior Loss Weight* - The weight to use when calculating prior loss. You probably want to leave this at 1.

*Center Crop* - Crop images if they aren't the right dimensions? I don't use this, and I recommend you just crop your
images "right".

*Pad Tokens* - Pads the text tokens to a longer length for some reason.

*Shuffle Tags* - Enable this to treat input prompts as a comma-separated list, and to shuffle that list, which can lead
to better editability.

*Max Token Length* - raise the tokenizer's default limit above 75. Requires Pad Tokens for > 75.

*AdamW Weight Decay* - The weight decay of the AdamW Optimizer used for training. Values closer to 0 closely match your training dataset, and values closer to 1 generalize more and deviate from your training dataset. Default is 1e-2, values lower than 0.1 are recommended.

## Concepts

The UI exposes three concepts, which seemed like a reasonable number of items to train on at once.

If you wish to use more than three concepts at once, you can ignore this section entirely, and instead use
the "Use Concepts List" option from the Miscellaneous section under the Parameters tab.


### Concept Parameters

Below is a list of the various parameters that can be used to train a concept.

*Maximum Training Steps* - The total number of lifetime training steps to train the concept until. Leave at -1 to use
the global value.

*Dataset Directory* - The directory in which the instance images are located.

*Classification Dataset Directory* The directory in which class images are stored. Leave empty to save to model
directory.

#### Filewords

The below values will be used in conjunction with the [filewords] tag in prompts to append/remove tags. See the
'Using [filewords]' section below for more information.

*Instance Token* The unique identifier for your subject. (sks, xyz). Leave blank for fine-tuning.

*Class Token* What your subject is. If a xyz is a person, this could be person/man/woman.

### Prompts

*Instance Prompt* - A prompt used for your instance images. Use [filewords] to insert or combine existing tags with
tokens.

*Class Prompt* - A prompt used for generating and training class images. Use [filewords] to insert or combine existing
tags with tokens.

*Classification Image Negative Prompt* - When generating class images, this is the negative prompt that will be used to
guide image generation.

*Sample Image Prompt* - A prompt used when generating sample images. Use [filewords] to insert or combine existing tags
with tokens.

*Sample Prompt Template File* - An existing txt file used to generate sample images. [filewords] and [names] will be
replaced with the instance token.

*Sample Image Negative Prompt* - When generating sample images, this is the negative prompt that will be used to guide
image generation.

### Image Generation

*Total Number of Class/Reg Images* - How many classification images will be generated. Leave at 0 to disable prior
preservation.

*Classification/Sample CFG Scale* - The Classifier Free Guidance scale to use when generating images.

*Classification/Sample Steps* - The number of steps to use when generating respective images.

*Number of Samples to Generate* - How many sample images to generate.

*Sample Seed* - A seed to use for consistent sample generation. Set to -1 to use a random seed.

#### Using [filewords]

Each concept allows you to use prompts from image filenames or accompanying text files for instance and class images.

To instruct the trainer to use prompts from existing files, use '[filewords]' (no quotes) for the instance/class/sample
prompts.

In order to properly insert and remove words from existing prompts, we need to let the trainer know what words indicate
what our subject name and class are.

To do this, we specify an instance and class token. If your subject were called 'zxy' and it was a man,
then your instance token would be 'zxy', and your class token would be 'man'.

Now, when building your respective prompts, the subject and class can be inserted or removed as necessary.

## Debugging

Here's a bunch of random stuff I added that seemed useful, but didn't seem to fit anywhere else.

*Preview Prompts* - Return a JSON string of the prompts that will be used for training. It's not pretty, but you can
tell if things are going to work right.

*Generate Sample Image* - Generate a sample using the specified seed and prompt below.

*Sample Prompt* - What the sample should be.

*Sample Seed* - The seed to use for your sample. Leave at -1 to use a random seed.

*Train Imagic Only* - Imagic is basically dreambooth, but uses only one image and is significantly faster.

If using Imagic, the first image in the first concept's Instance Data Dir will be used for training.


### Continuing Training

Once a model has been trained for any number of steps, a config file is saved which contains all of the parameters from
the UI.

If you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue
button next to the model name dropdown to load previous parameters.




